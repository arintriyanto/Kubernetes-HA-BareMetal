= High Availability Outside the Cloud

This section will cover how network traffic is routed in and out of the cluster in an HA way.

Of course, if you are running on AWS, your best option is to use your cloud provider's features for this.
Namely:

Elastic Load Balancers:: For incoming traffic into your Kubernetes cluster.

NAT Gateway:: For outgoing traffic, so that:
a. your traffic always appears as *originating from specific IPs* (useful, e.g. for firewall whitelisting)
b. you can keep all your EC2 instances in a private subnet (so that they don't have a public IP)
but still give them internet access.


...BUT what if you can't / don't want to use these?



== Load Balancing Incoming Traffic

Ok, let's decompose this into the following 3 steps:

1.DNS => Host:: Resolve Host from DNS
2.Host => Kubernetes:: Forward traffic from host to Kubernetes cluster
3.Kubernetes => Your Service:: Map incoming request to your service running within Kubernetes

Our solution will be based on the
https://github.com/kubernetes/contrib/tree/master/service-loadbalancer[Kubernetes Service LoadBalancer].

_The Service LoadBalancer is essentially deployed as a Pod *within* the k8s cluster, so we will first
need to look at how an incoming request will reach that specific pod._


=== DNS Resolution (DNS => Host)

This actually turns out to be much simpler than I would have thought and basically boils down to this:

[literal]
Add an `A` record to your DNS with the IP of every host that you will deploy the Service LoadBalancer pod on.

By adding the multiple `A` records, you are telling DNS clients to essentially round robin between the
candidate hosts.

A detailed discussion on the number of hosts you will choose to deploy it on, is slightly beyond
the scope of this guide. Suffice it to say you should deploy to *at least* 2 hosts.

_Ok, our incoming request has now reached one of the hosts. What next?_

=== Forwarding to Kubernetes (Host => Kubernetes)

The idea is, again, quite simple:

Rather than opening a separate port on each worker node, through multiple `NodePort` services, one for
each of the different services we will have running within the K8s cluster, instead, we will have *a single
`NodePort` service*, for the Service LoadBalancer.

This way, we avoid port conflicts on the host (e.g. in the case where we're deploying multiple instances of the
same app) and access control also becomes much simpler (only one firewall port needs to be opened).

Here is an example of the service definition:

[source, yaml]
----
---
apiVersion: v1
kind: Service
metadata:
  name: lb-service
  namespace: kube-system
spec:
  type: NodePort
  selector:
    app: service-loadbalancer
  ports:
  - port: 80
    nodePort: 30800
    name: http
  - port: 443
    nodePort: 30443
    name: https
----

_With this, the incoming request for `app.under.my_domain.name:30800`, should now have entered the
Kubernetes cluster and we just need to find out where to route it internally._

=== Route Internally (Kubernetes => Service)

This is finally where the Service LoadBalancer is able to do its thing. The incoming request has
reached one of its pods and it now needs to look up how to route this service.

The Service LoadBalancer is actually already very powerful (even though it's still under WIP) and
supports many different ways to do this, but I will be focusing on the `Name-based virtual hosting`.

NOTE: The Service LoadBalancer readme currently states that this is an undocumented feature, but I
 found it pretty easy to get it to work.

Let's consider an example to help you understand the differences involved.

Deploy Grafana and expose it not as `NodePort`, but rather through the Service LoadBalancer's
Name-based virtual hosting.

Traditionally, you would expose Grafana like so:

[source, yaml]
----
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
  labels:
    app: grafana
    component: core
spec:
  type: NodePort
  ports:
    - port: 3000
      nodePort: 30000
  selector:
    app: grafana
    component: core
----

This would open up port 30000 on the worker node and would forward incoming requests from that port
directly to Grafana.

Instead, using the Service LoadBalancer, we don't need to expose that port:

[source, yaml]
----
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
  labels:
    app: grafana
    component: core
  annotations:
    serviceloadbalancer/lb.host: grafana.under.my_domain.name:30800
spec:
#  type: NodePort
  ports:
    - port: 3000
#      nodePort: 30000
  selector:
    app: grafana
    component: core
----

(_I've commented out the lines to help you spot the difference_)

=== Limitations

Compared to AWS ELB, this approach does not provide a solution for the health check mechanism bundled
into ELB. Combined with an AWS Auto-Scaling Group, this can help you overcome node failures by
automatically destroying the failed instances and starting new ones.

I am not interested in this feature for the given use case and for the time being, so I am explicitly
excluding it from my scope.




== NATing outgoing traffic

As I mentioned above, there are 2 main reasons (that I know) for which you would need to use NAT for
your outgoing network traffic.

1.Static Origin IPs:: Your traffic always appears as *originating from specific IPs* (useful,
e.g. for firewall whitelisting)

2.Private Subnets:: You can keep all your EC2 instances in a private subnet (so that they don't have
a public IP) but still give them internet access.

=== High Availability NAT

Even though I am most certainly NOT a networks expert, from what I've gathered in the past couple of
days, an HA NAT deployment consists of the following components:

NAT:: A set of nodes that implement NAT, each with its own public static IP address (no pun intended).
Internet-bound traffic from the internal network is routed to these nodes, where NAT is applied
before the traffic is forwarded.

Routing Table:: A software-defined routing table for each of the private network subnets. This way the
traffic from the Kubernetes nodes who are running in the private subnets can reach a suitable NAT node,
so that it can then be forwarded to the internet.

Health check & Fail-over:: Some entity that monitors the NAT nodes (failures, unreachable, etc.) and
modifies the above routing table, in case one of the nodes goes down.

=== HA NAT @ Bare Metal

In my particular bare metal scenario, there is actually no hard "business" need for me to provide NAT
as part of my deployment. If any NATing takes place, it happens elsewhere on the network.

I am simply assigned a *set of predefined public IPs* for my Kubernetes cluster nodes.
This solves #1 and #2 is not a particular concern as there is a separate firewall (with its own set of policies).

And this leads us to the first real-world solution for implementing NAT: Don't do it!
YAGNI - i.e. it's already taken care of, for you.

NOTE: Even though I've not had the use case yet, in the case where you would need to roll your own
HA NAT, your solution would need to take the 3 components from the above section into account:
*1.* Who does the actual NATing? *2.* How is the traffic routed from the private subnets to the NAT nodes
and *3.* How do you monitor #1 and how do you modify #2 ?

=== HA NAT @ AWS

Once on AWS, things are always a lot simpler... and more expensive!

==== AWS NAT Gateway

The most expensive solution is the AWS NAT Gateway. It covers everything you need and it only takes
a few clicks (or CLI/API calls) to set it up.

You simply create a new NAT gateway from the AWS console. Then go to the Route Table of your private
subnet, which probably looks something like:

`<your_private_ip_range> local`

and add a single entry:

`0.0.0.0 <nat_gateway_id>`

so that all non-local traffic will now go through the NAT gateway.

IMPORTANT: You will need one NAT gateway per AZ, so you'll need to repeat this process if you have
a different private subnet per AZ.

==== AWS NAT Instances

This solution is a sort of a roll-your-own, but-with-Amazon's-help type of solution and boils down
to the following:

NAT Instances::
Deploy an off-the-shelf NAT Instance (meaning they give you the AMI you need) per AZ.

Route Table::
Add an entry to the route table of each private subnet towards the EC2 instance id of the NAT
instance in the same AZ as the subnet.

Health-Check::
Use the EC2 instance `User Data` to add a bash script that allows the NAT instances to monitor
each other.

The approach is described in detail in a (rather old)
https://aws.amazon.com/articles/2781451301784570[AWS article].

IMPORTANT: Even though *considerably* cheaper, there are 2 important caveats to this approach from
our experience.

1. There is some sort of bug in the `nat_monitor.sh` script that can lead to both NAT instances
reaching a `STOPPED` state. All it takes is a simple "start" to get them going, but we did have
to install appropriate monitors in place for this.

1. When picking the EC2 instance size for your NAT instance, you'll be inclined to just go with
 `t2.nano`. Do take into account that the smaller instances have a considerably lower network
 bandwidth, so if you are experiencing some sort of bottleneck that you can't trace on the rest
 of your infra, you'll want to test that too!
